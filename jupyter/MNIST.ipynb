{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP_NUM = 4\n",
    "EPS = 0.1\n",
    "alpha = 0.25\n",
    "beta = 0.75\n",
    "NUM_CLASS = 10\n",
    "\n",
    "(features_tr, labels_tr), (features_tst, labels_tst) = tf.keras.datasets.mnist.load_data()\n",
    "features_tr = features_tr[...,np.newaxis]\n",
    "features_tst = features_tst[...,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = (np.random.rand(FP_NUM, *features_tr.shape[1:]) - .5) * 2 * EPS\n",
    "\n",
    "dy_train = -np.ones((NUM_CLASS, NUM_CLASS)) * alpha\n",
    "for i in range(NUM_CLASS):\n",
    "    dy_train[i, i] = beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "def nn(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(\n",
    "                32,\n",
    "                (5,5),\n",
    "                activation = 'relu',\n",
    "                input_shape = input_shape\n",
    "        ),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "\n",
    "        tf.keras.layers.Conv2D(\n",
    "                64,\n",
    "                (5,5),\n",
    "                activation = 'relu',\n",
    "                input_shape = features_tr[0][:,:,np.newaxis].shape\n",
    "        ),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(200, activation='relu'),\n",
    "        tf.keras.layers.Dense(200, activation='relu'),\n",
    "        tf.keras.layers.Dense(NUM_CLASS, activation=None)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def print_loss(epoch, data_loss, fp_loss, acc, sess, feed_dict):\n",
    "    test_data_loss, test_fp_loss, test_loss, test_acc = sess.run(\n",
    "        [loss_vanilla, loss_fp, loss, accuracy],\n",
    "        feed_dict=feed_dict\n",
    "    )\n",
    "    print('epoch', epoch, 'data loss %.6f fp loss %.6f total loss: %.6f accuracy: %.3f' % \\\n",
    "          (test_data_loss, test_fp_loss, test_loss, test_acc))\n",
    "\n",
    "def normalize(logits):\n",
    "    return logits / tf.sqrt(tf.reduce_sum(logits**2, axis=1))[:, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Build the graph\n",
    "features = tf.placeholder(tf.float32, (None,) + features_tr.shape[1:])\n",
    "labels = tf.placeholder(tf.int64, [None])\n",
    "dx_tf = tf.constant(dx, dtype=tf.float32)\n",
    "dy_tf = tf.constant(dy_train, dtype=dx_tf.dtype)\n",
    "batch_size = tf.placeholder(tf.int64)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "batched_dataset = dataset.batch(batch_size)\n",
    "\n",
    "iterator = batched_dataset.make_initializable_iterator()\n",
    "\n",
    "features_batch, labels_batch = iterator.get_next()\n",
    "\n",
    "labels_oh = tf.one_hot(labels_batch, NUM_CLASS)\n",
    "\n",
    "network = nn(features_batch.shape[1:]) \n",
    "\n",
    "logits = network((features_batch + tf.random.uniform(tf.shape(features_batch)))/256)\n",
    "\n",
    "loss_vanilla = tf.losses.softmax_cross_entropy(labels_oh, logits)\n",
    "prediction = tf.argmax(tf.nn.softmax(logits), axis=1)\n",
    "prediction_correct = tf.equal(prediction, labels)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction_correct, tf.float32))\n",
    "\n",
    "perturbed_input = tf.reshape(\n",
    "    features_batch[tf.newaxis] + dx_tf[:, tf.newaxis],\n",
    "    (-1,) + features_tr.shape[1:]\n",
    ")\n",
    "fp_logits = tf.reshape(\n",
    "    network(perturbed_input),\n",
    "    (FP_NUM, tf.shape(labels_batch)[0], NUM_CLASS)\n",
    ")\n",
    "\n",
    "FxDx = normalize(fp_logits) - normalize(logits) # the paper shows it in <- this order but it's then not optimizable\n",
    "# FxDx = normalize(logits) - normalize(fp_logits)\n",
    "\n",
    "\n",
    "Dy = tf.gather(dy_tf, labels_batch)\n",
    "Dy = tf.stack([Dy] * FP_NUM)\n",
    "\n",
    "loss_fp = tf.reduce_mean((Dy - FxDx)**2)\n",
    "# loss_fp = tf.losses.mean_squared_error(Dy, FxDx)\n",
    "loss = loss_vanilla# + loss_fp\n",
    "train = tf.train.AdamOptimizer(.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# print_loss(0, loss_vanilla, loss_fp, accuracy, sess, feed_dict={})\n",
    "for epoch in range(1):\n",
    "    sess.run(iterator.initializer, {labels:labels_tr, features:features_tr, batch_size:500})\n",
    "    batch_num = 0\n",
    "    while True:\n",
    "        try:  \n",
    "            print(batch_num)\n",
    "            sess.run([train])\n",
    "            batch_num += 1\n",
    "            \n",
    "#             print_loss(epoch + 1, loss_vanilla, loss_fp, accuracy, sess, feed_dict={})\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.mgrid[-8:8:0.04, -8:8:0.03]\n",
    "\n",
    "# plot decision boundaries\n",
    "prediction_np = sess.run(prediction, feed_dict={\n",
    "                  features: np.dstack((x, y)).reshape((-1, 2))})\n",
    "plt.contourf(x, y, prediction_np.reshape(x.shape), cmap='gray')\n",
    "plt.scatter(features_training_np[:, 0],\n",
    "            features_training_np[:, 1], c=labels_training_np)\n",
    "# for i in range(FP_NUM):\n",
    "#     plt.arrow(-3, -3, 100 * dx[i, 0], 100 * dx[i, 1], color='r')\n",
    "plt.show()\n",
    "\n",
    "t1 = tf.expand_dims(FxDx, 2)\n",
    "t2 = t1 - dy_tf\n",
    "t3 = t2 ** 2\n",
    "t4 = tf.reduce_sum(t3, axis=-1)\n",
    "t5 = (1.0 / FP_NUM) * tf.sqrt(t4)\n",
    "t6 = tf.transpose(t5, [1, 2, 0])\n",
    "t7 = tf.reduce_sum(t6, axis=-1)\n",
    "t8 = tf.reduce_min(t7, axis=-1)\n",
    "\n",
    "# plot fingerprint loss\n",
    "x, y = np.mgrid[-8:8:0.05, -8:8:0.05]\n",
    "dissimilarities = sess.run(t8, feed_dict={features: np.dstack(\n",
    "    (x, y)).reshape((-1, 2)), dx_tf: dx[:, None, :]})\n",
    "plt.contourf(x, y, np.clip(dissimilarities.reshape(x.shape), 0, 1))\n",
    "plt.colorbar()\n",
    "plt.scatter(features_training_np[:, 0],\n",
    "            features_training_np[:, 1], c=labels_training_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY FINGERPRINTS\n",
    "\n",
    "\n",
    "fxdx, dy = sess.run([FxDx, Dy], feed_dict=training_dict)\n",
    "\n",
    "\n",
    "i = 3\n",
    "print(dy[:, i])\n",
    "print(fxdx[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.sum((dy - fxdx)**2, axis=(0,2)), log=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
