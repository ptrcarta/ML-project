{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataset\n",
    "\n",
    "N = 10000\n",
    "norm00 = np.random.multivariate_normal([-3, -3], [[1, 0], [0, 1]], size=N // 2)\n",
    "norm11 = np.random.multivariate_normal([3, 3], [[1, 0], [0, 1]], size=N // 2)\n",
    "\n",
    "labels_np = np.int64(np.hstack((np.zeros(norm00.shape[0]),\n",
    "                                np.ones(norm11.shape[0])\n",
    "                                )))\n",
    "features_np = np.float32(np.vstack((norm00, norm11)))\n",
    "data = np.hstack([features_np, labels_np[:, None]])\n",
    "np.random.shuffle(data)\n",
    "\n",
    "labels_np, features_np = np.intp(data[:, -1]), np.float32(data[:, :2])\n",
    "\n",
    "cut = N * 2 // 3\n",
    "features_training_np, labels_training_np = features_np[:cut], labels_np[:cut]\n",
    "features_testing_np, labels_testing_np = features_np[cut:], labels_np[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the fingerprints\n",
    "FP_NUM = 4\n",
    "EPS = 0.06\n",
    "alpha = 0.25\n",
    "beta = 0.75\n",
    "NUM_CLASS = 2\n",
    "dx = (np.random.rand(FP_NUM, features_np.shape[1]) - .5) * 2 * EPS\n",
    "\n",
    "dy_train = -np.ones((NUM_CLASS, NUM_CLASS)) * alpha\n",
    "for i in range(NUM_CLASS):\n",
    "    dy_train[i, i] = beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "def nn(input_shape):\n",
    "    model = tf.keras.Sequential((\n",
    "        tf.keras.layers.Dense(\n",
    "            200, activation='relu', input_shape=input_shape,\n",
    "            kernel_initializer=tf.initializers.truncated_normal,\n",
    "            bias_initializer=tf.initializers.truncated_normal\n",
    "        ),\n",
    "        tf.keras.layers.Dense(\n",
    "            200, activation='relu',\n",
    "            kernel_initializer=tf.initializers.truncated_normal,\n",
    "            bias_initializer = tf.initializers.truncated_normal\n",
    "        ),\n",
    "        tf.keras.layers.Dense(NUM_CLASS, activation=None,\n",
    "            kernel_initializer=tf.initializers.truncated_normal,\n",
    "            bias_initializer = tf.initializers.truncated_normal\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    return model\n",
    "\n",
    "def print_loss(epoch, data_loss, fp_loss, acc, sess, feed_dict):\n",
    "    test_data_loss, test_fp_loss, test_loss, test_acc = sess.run(\n",
    "        [loss_vanilla, loss_fp, loss, accuracy],\n",
    "        feed_dict=feed_dict\n",
    "    )\n",
    "    print('epoch', epoch, 'data loss %.6f fp loss %.6f total loss: %.6f accuracy: %.3f' % \\\n",
    "          (test_data_loss, test_fp_loss, test_loss, test_acc))\n",
    "\n",
    "def normalize(logits):\n",
    "    return logits / tf.sqrt(tf.reduce_sum(logits**2, axis=1))[:, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Build the graph\n",
    "features_tf = tf.placeholder(features_np.dtype, [None, 2])\n",
    "labels = tf.placeholder(labels_np.dtype, [None])\n",
    "dx_tf = tf.placeholder(features_np.dtype, [FP_NUM, None, 2])\n",
    "dy_tf = tf.constant(dy_train, dtype=dx_tf.dtype)\n",
    "\n",
    "labels_oh = tf.one_hot(labels, NUM_CLASS)\n",
    "\n",
    "network = nn(features_np.shape) \n",
    "\n",
    "logits = network(features_tf)\n",
    "\n",
    "loss_vanilla = tf.losses.softmax_cross_entropy(labels_oh, logits)\n",
    "prediction = tf.argmax(tf.nn.softmax(logits), axis=1)\n",
    "prediction_correct = tf.equal(prediction, labels)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction_correct, tf.float32))\n",
    "\n",
    "fp_logits = network(features_tf + dx_tf)\n",
    "\n",
    "# FxDx = normalize(fp_logits) - normalize(logits) # the paper shows it in <- this order but it's then not optimizable\n",
    "FxDx = normalize(logits) - normalize(fp_logits)\n",
    "\n",
    "\n",
    "Dy = tf.gather(dy_tf, labels)\n",
    "Dy = tf.stack([Dy] * FP_NUM)\n",
    "\n",
    "loss_fp = tf.reduce_mean((Dy - FxDx)**2)\n",
    "# loss_fp = tf.losses.mean_squared_error(Dy, FxDx)\n",
    "loss = loss_vanilla + 1e3*loss_fp\n",
    "train = tf.train.AdamOptimizer(.0001).minimize(loss)\n",
    "\n",
    "\n",
    "\n",
    "training_dict = {features_tf: features_training_np,\n",
    "                 labels: labels_training_np,\n",
    "                 dx_tf: dx[:, np.newaxis, :]}\n",
    "\n",
    "testing_dict = {features_tf: features_testing_np,\n",
    "                labels: labels_testing_np,\n",
    "                dx_tf: dx[:, np.newaxis, :]}\n",
    "\n",
    "mini_dict = {\n",
    "    features_tf: features_testing_np[:10],\n",
    "    labels: labels_testing_np[:10],\n",
    "    dx_tf: dx[:, np.newaxis, :]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print_loss(0, loss_vanilla, loss_fp, accuracy, sess, feed_dict=training_dict)\n",
    "for epoch in range(10000):\n",
    "    sess.run([train], feed_dict=training_dict)\n",
    "    if epoch % 10 == 0:\n",
    "        print_loss(epoch + 1, loss_vanilla, loss_fp, accuracy, sess, feed_dict=training_dict)\n",
    "#         print('\\t', end='')\n",
    "#         print_loss(epoch + 1, loss_vanilla, loss_fp, accuracy, sess, feed_dict=testing_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.mgrid[-8:8:0.04, -8:8:0.03]\n",
    "\n",
    "# plot decision boundaries\n",
    "prediction_np = sess.run(prediction, feed_dict={\n",
    "                  features_tf: np.dstack((x, y)).reshape((-1, 2))})\n",
    "plt.contourf(x, y, prediction_np.reshape(x.shape), cmap='gray')\n",
    "plt.scatter(features_training_np[:, 0],\n",
    "            features_training_np[:, 1], c=labels_training_np)\n",
    "# for i in range(FP_NUM):\n",
    "#     plt.arrow(-3, -3, 100 * dx[i, 0], 100 * dx[i, 1], color='r')\n",
    "plt.show()\n",
    "\n",
    "t1 = tf.expand_dims(FxDx, 2)\n",
    "t2 = t1 - dy_tf\n",
    "t3 = t2 ** 2\n",
    "t4 = tf.reduce_sum(t3, axis=-1)\n",
    "t5 = (1.0 / FP_NUM) * tf.sqrt(t4)\n",
    "t6 = tf.transpose(t5, [1, 2, 0])\n",
    "t7 = tf.reduce_sum(t6, axis=-1)\n",
    "t8 = tf.reduce_min(t7, axis=-1)\n",
    "\n",
    "# plot fingerprint loss\n",
    "x, y = np.mgrid[-8:8:0.05, -8:8:0.05]\n",
    "dissimilarities = sess.run(t8, feed_dict={features_tf: np.dstack(\n",
    "    (x, y)).reshape((-1, 2)), dx_tf: dx[:, None, :]})\n",
    "plt.contourf(x, y, np.clip(dissimilarities.reshape(x.shape), 0, 1))\n",
    "plt.colorbar()\n",
    "plt.scatter(features_training_np[:, 0],\n",
    "            features_training_np[:, 1], c=labels_training_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFY FINGERPRINTS\n",
    "\n",
    "\n",
    "fxdx, dy = sess.run([FxDx, Dy], feed_dict=training_dict)\n",
    "\n",
    "\n",
    "i = 3\n",
    "print(dy[:, i])\n",
    "print(fxdx[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.sum((dy - fxdx)**2, axis=(0,2)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
